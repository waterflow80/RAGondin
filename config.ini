[embedder]
type = huggingface
name = HIT-TMG/KaLM-embedding-multilingual-mini-v1
; powerful multilingual model => More language => token's decomposed heavily => More shit to embed => HIT-TMG/KaLM-embedding-multilingual-mini-v1(More params) holds better than Alibaba 
; Alibaba-NLP/gte-multilingual-base
; jinaai/jina-embeddings-v3
; Alibaba-NLP/gte-Qwen2-1.5B-instruct
; HIT-TMG/KaLM-embedding-multilingual-mini-v1
; thenlper/gte-base

[llm]    
base_url = https://chat.ai.linagora.exaion.com/v1/
name = meta-llama-31-8b-it
;Maximum number of tokens to generate.
max_tokens = 1024
temperature = 0.0

[vectordb]
host = localhost
port = 6333
connector_name = qdrant
collection_name = docs_vdb
hybrid_mode = True

[chunker]
; name can be recursive_splitter or semantic_splitter
name = semantic_splitter
;chunk_size = 1000
;chunk_overlap = 100
min_chunk_size = 1000
breakpoint_threshold_amount = 95

[retriever]
; type can be single, hyde, multiQuery
type = multiQuery
criteria = similarity
top_k = 6
; extra_params is essentially for multiQuery retriever
extra_params = {'k_queries': 3}

[reranker]
;if you don't want use the reranker, don't put any value
;colbert-ir/colbertv2.0
model_name = jinaai/jina-colbert-v2 
;number of docs to return after reranking
top_k = 5

[rag]
; mode can be 'SimpleRag' or 'ChatBotRag'
mode = ChatBotRag
; sys_template = rag_sys_prompt_template.txt
; chat_history_depth is use in case of 'ChatBotRag'
chat_history_depth = 4

[verbose]
verbose = True
level = INFO

[prompt]
rag_sys_pmpt = rag_sys_prompt_template.txt
context_pmpt_tmpl = contextualize_prompt_template.txt